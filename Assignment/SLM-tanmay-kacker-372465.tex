\documentclass{article}
\usepackage[a4paper, margin=20mm]{geometry}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage[numbered,framed]{mcode}
\usepackage{multicol}
\usepackage{upgreek}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xfrac}
\usepackage{float}
\usepackage[linguistics]{forest}
\usepackage{glossaries}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber, autocite=superscript, sorting=ynt]{biblatex}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{todonotes}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\parindent}{4em}
\setlength{\parskip}{1em}
\graphicspath{ {./media/} }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\addbibresource{references.bib}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\mean}[1]{\langle #1\rangle}
\newcommand{\matr}[1]{\underline{\mathbf{#1}}}
\newcommand{\lapl}{\bm{\triangle}}
\newcommand{\jacb}{\matr{\mathcal{J}}}
\newcommand{\hess}{\matr{\mathcal{H}}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\setlength\parindent{0pt}
\title{Statistical Learning Methods Assignment 2021/22}
\author{Tanmay Kacker - 372465 \\ tanmay.kacker.465@cranfield.ac.uk \\ Applied Artificial Intelligence MSc}
\date{Autumn 2021}
\begin{document}
\maketitle
\newpage
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction to the problem, describing the context. General data regarding the problem can be included in this part.
\subsection{Problem Brief}
Predictive maintenance refers to the set of techniques used to determine the operational condition of a system and estimate the maintenance time frame using predictive models.
This helps reduce the operational costs of maintenance and prevent untimely failures which could lead to extended downtimes.

An important aspect of predictive maintenance is failure prediction using historical data.
Analysis of sensor and telemetry data can be used to predict the Time-To-Failure (TTF) of a system which allows for the planning of a maintenance schedule. This is particularly useful for aircraft manufacturers and operators.

The aim of this report is to describe the usage of statistical learning techniques to historical sensor measurement data to predict the future failures of engines. Machine learning techniques can be used to establish relationships between sensor measurements and historical failures.
The goal is to make the following predictions:
\begin{enumerate}[topsep=0pt]
	\item The Time-To-Failure (TTF) of an engine.
	\item Classify whether an engine will fail in a given time period.
\end{enumerate}

\subsection{Data}
The data used for this report is a subset of a larger dataset generated by Microsoft and consists of run-to-failure scenarios for a number of aircraft engines.
The following data files were provided for building the predictive maintenance models:
\begin{enumerate}[topsep=0pt]
	\item \texttt{train\_selected.csv} - contains the historical sensor measurements and failure data for multiple engines over cycles of operation
	\item \texttt{test\_selected\_ttf.csv} - contains the historical sensor measurements and failure data for multiple engines at a randomly selected cycle of operation. This will be used to quantify the accuracy of the models.
\end{enumerate}

A detailed analysis of the data will be provided as a part of the data pre-processing and exploratory data analysis (EDA) sections of the report. 
\subsection{Problem Formulation}
In order to achieve the goals of the report supervised learning techniques will be used to establish relationships between sensor measurements and historical failures.
The two requirements of the problem will be formulated as follows:
\begin{enumerate}[topsep=0pt]
	\item Regression models to predict the continuous dependent variable \texttt{TTF}
	\item Binary classification models to predict whether an engine will fail in a given time period.
\end{enumerate}
\subsection{Development Environment}
\begin{enumerate}[topsep=0pt]
	\item Integrated Development Environment with \texttt{Visual Studio Code (1.62.3)} and \texttt{Python (3.9.1)}
	\item Data processing with \texttt{Pandas (1.3.4)} and \texttt{Numpy(1.21.3)}
	\item Data visualisation with \texttt{Seaborn (0.11.2)} and \texttt{Yellowbrick (1.3.post1)}
	\item Machine learning with \texttt{Scikit\_learn (1.1.dev0)}
\end{enumerate}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data pre-processing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Summary}
The training and testing dataset, provided as CSV files, were imported into a dataframe using the \texttt{Pandas} library.
Printing the samples of the dataset using \texttt{head} and \texttt{tail} functions makes it easier to understand the structure of the data.
The columns from the original dataset were renamed to make the data more readable.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{image1.png}
	\caption{First and last 3 rows in the training dataset}
\end{figure}

The dataset has 20,631 rows and comprises the following columns:
\begin{itemize}[topsep=0pt]
	\item \texttt{ID} - the unique identifier for each engine ranging from 1 to 100.
	\item \texttt{CYCLE} - The cycle of operation for the engine ranging from 1 to the cycle where failure happened.
	\item \texttt{S1, S2, S3 and S4} - the sensor readings for the engine in the cycle.
	\item \texttt{TTF} - the number of remaining cycles before the engine fails.
	\item \texttt{TTF\_LABEL} - 1 if the engine will fail in the cycle, 0 otherwise. An engine is flagged as 1 if the number of remaining cycles is less than or equal to 30.
\end{itemize}
\subsubsection{Test Data}
The test dataset was created as a hold out set and consists of 100 rows where each row consists of sensor measurements randomly selected from the engines in the training dataset.
The expected regression values \texttt{TTF} and classification labels \texttt{label\_bnc} are also provided.
\subsection{Missing values}
\begin{wrapfigure}[8]{r}{0.4\textwidth}
	\vspace{-45pt}
	\includegraphics[scale=0.6]{image2.png}
	\caption{Count of missing values in the dataset}
\end{wrapfigure}
Missing values in the dataset can hinder in the analysis of the data and the operation of the predictive models.
If there are missing values in the dataset, either the data can be imputed or the data points can be dropped if the count is below a certain threshold.
The dataset was checked for missing values using the \texttt{info} function from the \texttt{Pandas} library.

As can be seen from the output in Figure 2, there are no missing values in the dataset.
\newpage
\subsection{Outlier Detection}
Outlier are points in the dataset that significantly distant from the rest of the data or the inliers.
They can greatly hamper the accuracy of the predictive models. In order to check if the data has outlier, the \texttt{boxplot} function from the \texttt{Seaborn} library was used.
The box and whisker plot are a non-parametric method to visualize the distribution of the data where the box starts at the first quartile and ends at the third quartile. The middle line represent the median or the 50th percentile.
The whisker extend to 1.5 times the interquartile range (IQR) on each sides of the box. If points are outside the box, they are plotted individually and are considered as outliers.

It can be seen from the box plots in Figure 3 that each of the sensor readings have a certain amount of outliers. These will have to be identified and removed to improve the accuracy of the predictive models.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{image4.png}
	\caption{Box plots of sensors showing distribution of sensor readings and outliers}
\end{figure}

To quantitatively identify the outliers, the following univariate and multivariate statistical tests were used:
\begin{itemize}[topsep=0pt]
	\item \texttt{Numeric Outlier Technique} - A non-parametric technique which quantifies the approach presented in the box and whisker plot above. Any data point outside the sum of upper or lower quartile plus or minues 1.5 times the interquartile range is considered an outlier.
	      Here, 1.5 is used as the IQR multiplier. 427 rows were identified as outliers which is around 2\% of the dataset.
	\item \texttt{Z-score} - The Z-score is a parametric method that indicates the number of standard deviations a data point is away from the mean. The absolute value of the Z score should be greater than a certain threshold for a data point to be deemed as an outlier.
	      Here, the threshold was set to 3. 115 rows were identified as outliers which is around 0.6\% of the dataset.
	\item \texttt{Mahalanobis Distance} - A multivariate generalization of calculating the distance of a point from its mean in standard deviations using the covariance matrix of the component variables. A brief description of the outlier detection technique can be found in the next subsection.
	\item \texttt{Cook's distance} - Cook's distance is especially relevant when doing ordinary least squares regression and measures the influence of a data point to the results using the residuals and the mean squared error. A brief description of the outlier detection technique can be found in the last subsection.
\end{itemize}
\subsubsection{Mahalanobis Distance}
\todo[inline]{Add details and plot for MD or remove subsection and add to the list.}
\subsubsection{Cook's Distance}
As mentioned above, Cook's distance is a measure of the influence of a data point on the estimated OLS regression coefficients.
It is a function of the standardized residuals and the leverage associated with each data point. Once the influence is computed, outliers are identified by setting the threshold as \(\frac{4}{n}\) where \texttt{n} is the number of data points. 
In order to visualize the influence, the function \texttt{CooksDistance} was used from the \texttt{Yellowbrick} library. The function plots as a stem plot where the x-axis is each data point and the y-axis is the corresponding influence or the Cook's distance as shown in Figure 4.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{image3.png}
	\caption{Detecting outliers in the dataset using Cook's distance}
\end{figure}

As seen above, 4.38\% of the data points are outliers which corresponds to 904 rows having more than \num{1.94e4} influence where the latter is the threshold for identifying outliers. Removing the outliers from the dataset will improve the accuracy of the linear regression model which will be seen in the regression section.
The dataset was pruned to remove the outliers using the \texttt{drop} function from the \texttt{Pandas} library and stored as a CSV file for further usage.
\subsection{Feature extraction}
\todo[inline]{Test rolling mean and standard deviation}
\subsection{Summary}
\begin{itemize}
	\item The dataset has four columns with sensor measurements - \texttt{S1, S2, S3 and S4} along with engine identifier \texttt{ID} and current cycle of operation \texttt{CYCLE}.
	\item There are also regression values for the engine failure time \texttt{TTF} and the classification labels for the engine failure \texttt{TTF\_LABEL}.
	\item There are 100 engines with 20,631 and 100 rows in the training and test datasets respectively. 
	\item The dataset has no missing values.
	\item Outliers were detected in the dataset and subsequently removed.
	\item 4.38\% of the data points were identified as outlier by Cook's distance and were specifically removed for OLS regression.
\end{itemize}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exploratory Data Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Descriptive Statistics}
It was previously identified that the training dataset has \num{20,631} rows with 8 columns corresponding to the data of 100 engines. The sensor measurements are continuous values while the dataset does not contain any categorical values except for the classification labels.
The columns \texttt{ID and CYCLE} serve as the indexing columns and their usage in training the model could lead to target leakage. We shall avoid using these columns unless the hypothesis is invalidated.

\subsubsection{Central Tendency and Dispersion}
To generate the summary of central tendency and variability of the data, the \texttt{describe} function from the \texttt{Pandas} library was used.
Since for each engine there is a row for each cycle of operation, the dataset was grouped by the engine identifier and the maximum value of cycle was extracted for generating the summary statistics. This corresponds to the maximum cycle of operation for each engine or the \texttt{TTF}.
The following key observations were made about the \texttt{ID and CYCLE} columns:
\begin{itemize}[topsep=0pt]
	\item The values of \texttt{ID} range from 1 to 100 but the mean and quartiles do not line up indicating each engine has a variable maximum number of cycles.
	\item The above disparity is elucidated by the statistics of the aggregated cycle column where the average engine failure is around 199 - 206 cycles.
	\item Minimum number of cycles that an engine ran for before failure is 128 and the maximum is 306 with a standard deviation of around 46.34 cycles.
\end{itemize}
The above statistics can be further verified by plotting the histogram of the aggregated cycle column using the \texttt{histplot} function from the \texttt{Seaborn} library.
\todo{Try wrap figure}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{image14.png}
	\caption{Histogram of the maximum cycle of operation for each engine}
\end{figure}

Understanding the variability of the sensor measurements is extremely important to identify the features to be selected. If a feature does not show any significant variability, it is likely to not affect the dependent variable.
The standard deviation of the sensor measurement was calculated using the \texttt{std} function from the \texttt{Pandas} library and plotted using the \texttt{plot} function as shown in Figure 6.

\texttt{S1} has a significant standard deviation of around 9 while \texttt{S2} and \texttt{S4} have a standard deviation of around 0.89 and 0.74 respectively.
\texttt{S3} has the least standard deviation of around 0.26. Analysis of the correlation of the sensor measurements with the dependent variable \texttt{TTF} would lead to the selected set of features.
\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{image5a.png}
		\caption{Standard deviation of the sensor variables}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{image5b.png}
		\caption{Logarithmic standard deviation of the sensor variables}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{Variability of sensor measurements}
	\label{fig:fig}
\end{figure}

\subsubsection{Correlation}
\label{sec:corr}
Correlation is a measure of the linear dependence between two variables. A greater correlation between dependent and independent variables indicates a strong predictive capability.
Whereas multicollinearity between the independent variables is not desirable as it can lead to overfitting and increase the dimensionality of the model. Regression models are specifically sensitive to correlated variables.

To analyse the correlation between the variables, Pearson's correlation coefficient was used from the \texttt{Pandas} library and a heat map was generated using the \texttt{heatmap} function from the \texttt{Seaborn} library.
Pearson's R ranges from -1 to 1 with a value of 0 indicating no correlation. Other measures such as Spearman's R and Kendall's Tau yielded similar results.
The covariance of the independent variables was also calculated using the \texttt{cov} function from the \texttt{Pandas} library and plotted.
\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{image6.png}
		\caption{1a}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{image7.png}
		\caption{1b}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{plots of....}
	\label{fig:fig}
\end{figure}

The following are the major observations that are clearly visible from the correlation and covariance matrix in Figure 6:
\begin{itemize}[topsep=0pt]
	\item \texttt{S2} and \texttt{S4} are positively correlated with \texttt{TTF} with scores of 0.66 and 0.67 respectively.
	\item \texttt{S1} and \texttt{S3} are negatively correlated with \texttt{TTF} with scores of -0.68 and -0.7 respectively.
	\item (\texttt{S1}, \texttt{S3}) and (\texttt{S2}, \texttt{S4}) have significant positive correlations within each other with scores of 0.83 and 0.81 respectively.
	\item (\texttt{S1}, \texttt{S4}) and (\texttt{S3}, \texttt{S4}) have significant negative correlations within each other with scores of -0.82 and -0.85 respectively.
\end{itemize}
The correlation with the dependent variable is indicative of good predictive capability and is desirable.
However, the correlation between the independent variables can hamper the performance of the model and thus will have to be resolved.
These features will either have to be removed during the feature selection process or the redundancy could be reduced using principal component analysis (PCA) where the features are assimilated and dimensions are reduced.

\subsection{Visual EDA} \label{sec:visual-eda}
The pairwise relationships were visualized using the \texttt{pairplot} function from the \texttt{Seaborn} library.
The diagonal shows the univariate marginal distributions of the variables.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{image5.png}
	\caption{Pairwise relationships between dependent and independent variables}
\end{figure}

The significant observations that can be made from the plot in Figure 8 are:
\begin{itemize}
	\item All the dependent variables display to have a normal distribution which is desirable due to the assumption of normality in parametric methods.
	\item The correlation between the sensor measurements and \texttt{TTF} is apparent as computed in the correlation matrix.
	\item The correlation between the sensor measurements and \texttt{TTF} not linear but of the polynomial nature. 
	\item The linear correlation between the independent variables is apparent as computed in the correlation matrix.
\end{itemize}
\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=\linewidth]{image8.png}
		\caption{Measurements of S1}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\includegraphics[width=\linewidth]{image15.png}
		\caption{Measurements of S2}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{Plots of signals from Sensor 1 and 2 sampled from 10 random engines}
	\label{fig:fig}
\end{figure}

The correlation between sensor measurements and \texttt{TTF} is also visible in the plot of the measurements from Sensor 1 and 2 from 10 randomly sampled engines.
As the engines move towards failure, measurements from \texttt{S1} spike whereas \texttt{S2} tends to have lower values near failure.

Another interesting observation is that initially the measurements from the sensor tend to be constant and beyond a certain threshold is when the slope gets steeper.
This could be used to improve model accuracy via pruning the values before the threshold, which seems like somewhere between 150 and 125 cycles.

\subsubsection{Skewness and Kurtosis}
Since, the distribution of the dependent variables is normal, but there is some visible skewness, we need to analyse the extent of it and see if corrective measures need to be taken.
The skewness and kurtosis were calculated using the \texttt{skew} and \texttt{kurt} functions from the \texttt{Pandas}. The following was observed from the results:
\begin{itemize}
	\item \texttt{S1} and \texttt{S3} are positively skewed with scores of 0.44 and 0.47 respectively.
	\item \texttt{S2} and \texttt{S4} are negatively skewed with scores of -0.39 and -0.44 respectively.
	\item All the variables are mildly platykurtic with kurtosis in the range of -0.14 to -0.16.
\end{itemize}
Skewness can be fixed by transformations namely the box-cox transformation.
The skewness of the features is not significant enough to be corrected, and thus the features are not transformed.

\subsection{Classification}
\begin{wrapfigure}[12]{r}{0.5\textwidth}
    \vspace{-45pt}
	\includegraphics[scale=0.4]{image10.png}
	\caption{Visualization of classification imbalance in the training data}
\end{wrapfigure}
The target variable \texttt{TTF\_LABEL} is a categorical variable with two possible values - \texttt{0} and \texttt{1}. The target variable is used to classify if the engine will fail in a given period.
Checking the support for each class, we see that the number of instances of \texttt{0} is much higher than the number of instances of \texttt{1}. The value \texttt{0} of the classification label nearly accounts for 85\% of the dataset which can be seen in Figure 10.
This is known as class imbalance. This cane be corrected by performing rebalancing or stratified sampling. 
A consequence of this is that accuracy will not be a good measure of the model performance and hence will not be used for model selection and evaluation.
\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{image9.png}
		\caption{Scatter plot of S1 and S3 with TTF\_LABEL}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{image13.png}
		\caption{Parallel coordinates of sensors with TTF\_LABEL}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{Plots showing separability among the classes}
	\label{fig:fig}
\end{figure}
\todo{Describe classification little bit}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Regression Models - Estimating the Time-To-Failure (TTF)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear Regression}
Standard ordinary least squares (OLS) regression was used to set up the baseline model.
The initial model was trained on the original training dataset and then on the dataset which was normalized and had the outliers removed.
The model was evaluated using the \texttt{R2} and \texttt{root mean squared error (RMSE)} metrics.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{image19.png}
	\caption{First and last 3 rows in the training dataset}
\end{figure}

\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{image17.png}
		\caption{Scatter plot of S1 and S3 with TTF\_LABEL}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{image18.png}
		\caption{Parallel coordinates of sensors with TTF\_LABEL}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{Plots showing separability among the classes}
	\label{fig:fig}
\end{figure}
\todo{Describe the linear regression residuals}
\subsection{Polynomial Regression}
As mentioned in Section \hyperref[sec:visual-eda]{3.2}, the relationship between sensor measurements and \texttt{TTF} is non-linear and hence a polynomial regression model would perform better than linear models.
In order to test this hypothesis, polynomial features of the dependent variables were created and the model was trained on the dataset.
Hyperparameter tuning was done through Grid search to identify the best polynomial degree to use. In order to evaluate each candidate cross validation across 5 folds was used.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{image23.png}
	\caption{Grid search for finding the best polynomial degree}
\end{figure}

As seen in Figure 14, the best polynomial degree is 4. The polynomial features were created using the \texttt{PolynomialFeatures} class from the \texttt{sklearn} library.
The model was trained and evaluated using the \texttt{R2} and \texttt{root mean squared error (RMSE)} metrics.
\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1.2\linewidth]{image20.png}
		\caption{Scatter plot of S1 and S3 with TTF\_LABEL}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{image21.png}
		\caption{Parallel coordinates of sensors with TTF\_LABEL}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{Plots showing separability among the classes}
	\label{fig:fig}
\end{figure}

The following are the key observations from the result metrics and plots:
\begin{itemize}[topsep=0pt]
	\item The model had a R2 score of 0.812 and an RMSE of 18.099 on the training dataset.
	\item The model had a R2 score of 0.735 and an RMSE of 21.401 on the test dataset which is 14\% better than the baseline model.
	\item The residuals are randomly spread around the horizontal axis and are fairly normally distributed.
\end{itemize}
\todo{Describe the polynomial regression residuals}

\subsection{Random Forest Regression}
As mentioned in Section \hyperref[sec:corr]{3.1.2}, the sensor measurements exhibit collinearity which can hamper the performance of predictive models.
Random forest regression is a non-parametric method which is quite robust to correlated dependent variables and even outliers.
\texttt{RandomForestRegressor} from the \texttt{sklearn} library as the model of choice and hyperparameter tuning was done through Grid search with cross-validation across 5 folds.
Random forest has quite a few hyperparameters that need to be tuned to avoid overfitting where the effect of \texttt{n\_estimators, max\_features} and \texttt{max\_depth} is quite significant.
These determine the structure of the decision trees and the splitting criterion.
\begin{figure}[h]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{image24.png}
		\caption{Scatter plot of S1 and S3 with TTF\_LABEL}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{image25.png}
		\caption{Parallel coordinates of sensors with TTF\_LABEL}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{Plots showing separability among the classes}
	\label{fig:fig}
\end{figure}
The following are the key observations from the result metrics and plots:
\begin{itemize}[topsep=0pt]
	\item The model had a R2 score of 0.832 and an RMSE of 17.069 on the training dataset.
	\item The model had a R2 score of 0.727 and an RMSE of 21.717 on the test dataset which is 14\% better than the baseline model.
	\item The residuals are randomly spread around the horizontal axis and are fairly normally distributed.
\end{itemize}
\subsection{Conclusions}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classification Models - Predicting whether an engine will fail in a given time period}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Write your conclusions aligned with the problem context. 
\subsection{Summary}
% Describe what could be done in the future to improve the problem within the context.
\subsection{Next Steps}
% How could this problem be taken further?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\printbibliography
\section*{Appendix I}
\end{document}